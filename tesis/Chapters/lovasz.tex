
\part{Solvers}
\label{chap:2}


\chapter{Special Cases}

On this part we attack the main problem of SAT: explain the different techniques that can be applied.Onward we will see how it could be solved, and develop applied techniques. There are a lot of approaches to this problem and they differ on their way to attack it. We have to realise that three things are important to judge an algorithm.

\begin{itemize}
\item The simplicity: following Occam's razor, between two solutions that do not appear to be better or worse, one should choose the easiest one. This solution are far more comprehensible and tends to be more variable and adaptable for our problem. We should not despise an easy solution to a complex problem only because a far more difficult approach give slightly better results.

\item The complexity: and by that I mean its algorithmic ('Big O') complexity. It is important to get good running times in all cases and have an analysis of the worst cases scenario that the algorithm could have.

\item The efficiency: Some algorithms will have the same complexity as the most simple ones, but will use some plans to be able to solve most part of the cases fast (even in polynomial time). There are some cases that would make this algorithms be pretty slow, but more often than not a trade-off is convenient.
\end{itemize}


  On this chapter we are going to talk about solvability in special situations where we work with a restricted subset of formulas (more restricted than CNF formulas). We want to exploit special characteristics of these subsets in our favor in order to get a resolution without involving a complex exponential time algorithms (as the ones needed to solve SAT).\\


  The first section of this chapter will talk about combinatorics. We proceed to analyze solvability in special cases, i.e., algorithms  that work really well in formulas given that they satisfy some restriction.

\section{Satisfiability by Combinatorics}
\label{sec:combu<}

To get an intuition about how unsolvable clauses are, we gonna state some simple results about combinatorics and resolution. These techinques present some cases where we can solve the decision problem efficently, although more often that not we would not provide a satisfying assignment, i.e., we do not solve the function problem.


As there is no complete SAT-solver known to work in polymial time complexity, a polynomial increase does not affect overall the assymptotical complexity.\\

Firstly, it is easy to break a big clause on some smaller ones, adding one another on this way: Suppose we got two positive integers $n,m$ such that $m < n$ a clause $x_1\vee x_2 \vee ... \vee x_n$ we could split it into two parts $x_1\vee x_2  \vee ... \vee x_{m-1} \vee y, \neg y \vee x_m \vee ... \vee x_n$. Also given the same clause with a given length $n$ we could enlarge it one variable adding $ x_1 \vee ... \vee x_n \vee y$ and $ x_1 \vee ... \vee x_n \vee \neg y$ where $y$ is a new variable. Note that to enlarge a clause from a length $m$ to a length $n>m$ we would generate $2^{n-m}$ clauses.

\begin{proposition}
	Let $F$ be a $k$-CNF formula, if $|F| < 2^k$ then $F$ is satisfiable.
\end{proposition}
\begin{proof}
	Let $n = Var(F)$, it happens that $n > k$. For each clause $C \in F$ there are $2^{n-k}$ assignments that falsify $F$, so in total there could be strictly less than $2^k \cdot 2^{n-k} = 2^n.$ Therefore it exists an assignment that assigns all variables and not falsifies the formula $F$.
\end{proof}
\begin{proposition}
	Let $F=\{C_1,...,C_n\}$ be a CNF formula. If $\sum_{j=1}^m 2^{-|C_j|}<1,$ then $F$ is satisfiable.
\end{proposition}
\begin{proof}
	Enlarging clauses the way it is explained to the maximum length $k$ and applying the previous result.
\end{proof}

Following this idea we could define the weight of a clause $C\in F$ as $$\omega(C) = 2^{-|C|} $$
being this the probability that a uniform-random assignment violates this clause. 

\begin{corollary}
	For a formula in CNF, if the sum of the weights of the clauses is less than one then the formula is satisfiable.
\end{corollary}


% \begin{proof}
% For this task, we will give a probabilistic algorithm, only to prove that it will end with a big probability. Probabilistic (and heuristic) approaches to the problem would prove later on to be really useful. Let $F$ be a CNF formula regarded as a clause set.
% \end{proof}



\begin{definition}
Let $F$ be a CNF formula. It is said to be minimally unsatisfiable if:
\begin{itemize}
	\item $F$ is unsatisfiable.
	\item $F\backslash \{C\}$ is satisfiable $\forall C \in F$.
\end{itemize}
\end{definition}


Then the following proof will be shown as in \cite{schoning2013satisfiability}. For that we will need the well known Hall marriage theorem\cite{hall2009representatives}. A similar result is proved in Chapter 7 of \cite{marek2009introduction}, using the König theorem. Both versions take the idea of associate a graph with a set of clauses. 

\begin{definition}
  Let $G=(N,E)$ be a graph where $N$ is the set of nodes and $E$ the set of edges, represented as pair of nodes. Given $n \in X$,  the neighborhood of $n$, denoted as $\Gamma_G(n)$, is defined as:
	$$ \Gamma_G(n) = \{ n' \in X : n'\ne n, \exists e \in E \text{ such that } n,n' \in e\}$$
	
        Analogously, the inclusive neighborhood is defined as $\Gamma_G^+(n) = \Gamma_G(n) \cup \{n\}$. The neighborhood of a subset $W \subset X$ is defined as $\Gamma_G(X) = \Cup_{n\in X} \Gamma_G(n)$
\end{definition}

\begin{definition}
  Let $G$ be a finite bipartite graph with finite sets of vertices $X,Y$. A matching edge cover is a cover such that every vertex only participate in one edge)
\end{definition}

\begin{theorem}[Hall marriage graph version]
  Let $G$ be a finite bipartite graph with finite sets of vertices $X,Y$. There is a matching edge cover of $X$ if and only if $|W| \le |\Gamma_G(W)|$ for every $W\subset X$.  
\end{theorem}


\begin{lemma}
Let F be a CNF formula. If for every subset $G$ of $F$ it holds that $|G|\le |Var(G)|$, then $F$ is satisfiable.
\end{lemma}

\begin{proof}
  We will associate a bipartite graph with $F$:  $U, V$ be the two set of vertices where $U$ consists on the set of clauses and $V$ on the set of variables. There is an edge $(u,v)$ if $v$ takes part on $u$.

  By the marriage theorem every clause can be associated to a variable. Therefore we could make an assignment that take every variable associated to a clause to the value that the clause requires.
\end{proof}

This idea or neighbourhood in clause is important and curious. It defines a relation between clauses and give clauses resolution some nice graph-tools to work with.

\begin{proposition}
	If $F$ is minimally unsatisfiable, then $|F| > Var(F)$.
\end{proposition}

\begin{proof}
  Since $F$ is unsatisfiable, there must be a subset $G$ such that is maximal and satisfy $|G| > Var(G)$. If $G=F$ them the theorem is proved.


  Otherwise, let $H \subset F\backslash G$ be an arbitrary subset. If $|H| > |Var(H)\Var(G)|$ then $|G \cup H| > |Var(G\cup H)|$ and $G$ would not be maximal.  Therefore $F\G$ satisfies the condition of the lemma and is satisfiable using an assignment that does not use any variable $x \in Var(G)$. As $G$ is minimally unsatisfiable $G$ is satisfiable by an assignment $\beta$. We could then define an assignment:

  \[   
\gamma(x) = 
     \begin{cases}
       \beta(x) &\quad\text{if } x \in Var(G)\\
       \alpha(x) &\quad\text{otherwise.} \\ 
     \end{cases}
   \]
this assignment would satisfy F against the hypothesis. We proved $G=F$ by contradiction and therefore we proved the lemma.
\end{proof}




\section{Lovász Local Lemma}
We continue to prove an interesting lemma on the theoretical analysis of satisfiability problem: the Lovász Local Lemma (LLL). This lemma was first proven on 1972 by Erdös and Lovász while they were studying 3-coloration of hypergraphs. Then it was Moser which understood the relationship between  this result and the  constraint satisfaction problem. SAT could be regarded as the simplest of these problems. \\

%Incluir teorema pagina 20? 

 
This section is going to be based on the works of Moser, Tardos, Lovász and Erdös. As it will be shown, LLL is applicable to set a sufficient condition for satisfiability.  We will explain the lemma for theoretical purposes and prove the most general version, and give a constructive algorithm to solve a less general statement of the problem. The principal source of bibliography for the whole section would be Moser PhD. Thesis\cite{moser2009constructive}. \\ 



The main contribution of Moser's work to this problem is finding an efficient constructive algorithm to find what assignment satisfies the formula, given that $F$ satisfies the hypothesis of the lemma. Previously only probabilistic approaches had been successful.\\


The probabilistic method is a useful method to prove the existence of objects with an specific property. The philosophy beneath this type of proofs is the following: in order to prove the existence of an object we do not need to give the object, instead, we could just consider a random object in the space we are exploring an prove that the probability is strictly positive. Then we can deduce that an object with that property exists. It is not necessary to provide the exact value, bounding it by a constant greater that zero would be enough. \\

This technique was pioneered by Paul Erdös. LLL is an useful tool to prove lower bounds for probabilities that is commonly used to prove that a probaility is strictly positive.\\

This section will follow this order:
\begin{itemize}
	\item Present the notation and general expression for the LLL.
	\item Use the result to prove an interesting property on satisfiability on CNF.
	\item Prove the general result with the probabilistic result.
	\item Provide the more concise CNF-result with a constructive algorithm.
\end{itemize}


\subsection{First definitions}

We will work here with a very specific type of formulas.
\begin{definition}
	Let $C$ be a clause in $F$, the neighborhood of $C$, denoted as $\Gamma_F(C)$ as 
	$$ \Gamma_F(C) = \{ D \in F : D\ne C, Var(C) \cap Var(D) \ne \emptyset\}.$$
	
	Analogously, the inclusive neighborhood $\Gamma_F^+(C) = \Gamma(C) \cup \{C\}$. 
\end{definition}


Further on $\Gamma$ and $\Gamma^+$ will respectively denote inclusive or exclusive neighborhood on CNF formulas or graphs


\begin{definition}
	
	Two clauses are \emph{conflicting} if there is a variable that is required to be true in one of then and to be false in the other. $G_F^*$ is the graph such that there is an edge between $C$ and $D$ iff they \emph{conflict} in some variable.
	
\end{definition}
	



\begin{definition} Let $\Omega$ be a probability space and let 
$\mathcal{A} = \{A_1,...,A_m\}$ be arbitrary events in this space. We say that a graph $G$ on the vertex set $\mathcal{A}$ is a \emph{lopsidependency graph }for $\mathcal{A}$ if no event is more likely in the conditional space defined by intersecting the complement of any subset of its non-neighbors. In others words:

\[
 P\left(  A\ \Big | \bigcap_{B\in S} \overline{B} \right ) \le P(A) \ \ \ \ \quad \forall A \in \mathcal{A},\ \forall S \subset \mathcal{A} \backslash\Gamma_G^+(A) .
\]


If, instead of requiring the event to be more likely, we require it to be independent (i.e. to be equal in probability) the graph is called \emph{dependency graph}.

\end{definition}

\subsection{Statement of the Lovász Local Lemma}

\begin{theorem}[Lovász Local Lema]\label{LLL}
	Let $\Omega$ be a probability space and let 
$\mathcal{A} = \{A_1,...,A_m\}$ be arbitrary events in this space. Let $G$ be a lopsidependency graph for $\mathcal{A}$. If there exists a mapping $\mu:\mathcal{A} \to (0,1)$ such that 
$$
\forall A \in \mathcal{A} : P (A) \le \mu(A) \prod_{B\in\Gamma_G(A)} (1-\mu(B)),
$$

then $P\left ( \bigcap_{A\in \mathcal{A}} \overline{A}\right ) > 0$.\\
\end{theorem}

By considering the random experiment of drawing an assignment uniformly, with the event corresponding to violating the different clauses we could reformulate this result. The weight of each clause is the probability of violating each clause. Therefore, we can state a SAT-focused result.

\begin{corollary}[Lovász Local Lema for SAT]\label{LLLS}
	Let $F$ be a CNF formula. If there exists a mapping $\mu:F\to (0,1)$ that associates a number with each clause in the formula such that 
	
	$$
\forall A \in \mathcal{A} : \omega (A) \le \mu(A) \prod_{B\in\Gamma^*_G(A)} (1-\mu(B)),
$$
	then F is satisfiable.
\end{corollary}
\begin{proof}
	To prove the result it would only be necessary to show  that $ \Gamma^*$ is the lopsidependency graph for this experiment. Given $C \in F$ and $\mathcal{D}\subset F\backslash \Gamma_{G_F^*}(D)\ $(i.e. no $D \in  \mathcal{D}$ conflict with $C$). We want to check the probability of a random assignment falsifying $C$ given that it satisfies all of the clauses in $\mathcal{D}$, and prove that it is at most $2^{-|C|}$. \\ 
	
Let $\alpha$ be an assignment such that it satisfies $\mathcal{D}$ and violates $C$. We could generate a new assignment from $\alpha$ changing any value on $Var(C)$, and they still will satisfy $\mathcal{D}$ (as there are no conflict) so the probability is still at most $2^{-k}$. 

% REVISAR 

\end{proof}


The result that we will prove in a constructive way will be slightly more strict, imposing the condition not only in $\Gamma^*$ but in $\Gamma^+$ 


\begin{corollary}[Constructive Lovász Local Lema for SAT]\label{LLLSC}
	Let $F$ be a CNF formula. If there exists a mapping $\mu:F\to (0,1)$ that associates a number with each clause in the formula such that 
	
	$$
\forall A \in \mathcal{A} : \omega (A) \le \mu(A) \prod_{B\in\Gamma_G(A)} (1-\mu(B)),
$$
	then F is satisfiable.
\end{corollary}


In order to get a result easier to check we will present a new criteria. If $k\le 2$ the $k$-SAT problem is  polynomially solvable so we will not be interested on such formulas.

\begin{corollary}
	Let $F$ be a $k$-CNF with $k>2$ formula such that $\ \forall C \in F$ and $\ |\Gamma_F(C)|\le 2^k/e-1$ then $F$ is satisfiable.
      \end{corollary}
      
\begin{proof}
	
	 We will try to use [\ref{LLLSC}]. We will define such $\mu: F \to (0,1),$$\ \mu(C)=e\cdot 2^{-k}$. Let $C_0\in F$ be an arbitrary clause.
	 
	 \[
	 2^{-k}=\omega(C)\le  \mu(C) \prod_{B\in\Gamma_F(C)} (1-\mu(B)) = e2^{-k}(1-e 2^{-k})^{|\Gamma_F(C)|}.
	 \]
	 With the hypothesis
	 
	\[
		 2^{-k} \le  e 2^{-k}(1-e2^{-k})^{2^k/e-1},\]\[
		 1  \le e(1-e2^{-k})^{2^k/e-1}.
	\]
	
	Being famous that the convergence of the sequence $\{(1-e2^{-k})^{2^k/e-1}\}_k$ to $1/e$ is monotonically decreasing.\\
\end{proof}



\subsection{Nonconstructive proof of [\ref{LLL}]}

We explain the way Erdös, Lovász and Spencer originally proved the Lemma \cite{erdos1973problems} \cite{spencer1977asymptotic}. The write-up presented here will resemble the one done by
 \cite{moser2013exact}.\\



Thorough the proof we will use repeatedly the Chain Rule. It states that for any events $\{E_i\}_{i\in 1,...,r}$,

\[
P\left ( \bigcap_{i=1}^r E_1\right ) = \prod_{i=1}^rP \left( E_i \Big | \bigcap_{j=1}^{i-1} E_j\right).\\
\]

Further on this subsection we will consider  $\Omega$ to be a probability space and $\mathcal{A} = \{A_1,...,A_m\}$ to be arbitrary events in this space, $G$ to be a lopsidependency graph, and $\mu: \mathcal{A} \to (0,1)$ such that the conditions of the theorem are satisfied. We first prove an auxiliary lemma.

\begin{lemma} \label{LemaLLL}
Let $ A_0 \in \mathcal{A} $ and $\mathcal{H}\subset \mathcal{A} $. then 
\[
	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B}\right ) \le \mu(A) .
\]

		
\end{lemma}

\begin{proof}

The proof is by induction on the size of $|\mathcal{H}|$. The case $H=\emptyset$ follows from the hypothesis easily:

$$ 
	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B}\right ) =  P(A) \le^{1.}   \mu(A) \prod_{B\in\Gamma^*_G(A)} (1-\mu(B)) \le^{2.} \mu(A) .$$

Where 1. uses the hypothesis and 2. uses that $0 < \mu(B) < 1$. Now we suppose that $|\mathcal{H}|=n$ and that the claim is true for all $\mathcal{H}'$ such that $|\mathcal{H}'|<n$. We distinguish two cases. The induction hypothesis will not be necessary for the first of them
\begin{itemize}
\item When $\mathcal{H} \cap \Gamma^*_G(A) = \emptyset$ then  $	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B}\right ) = 0 \le P(A)$ by definition of $\Gamma_G^*$ and $P(A) \le \mu(A)$ by definition of $\mu$.
\item Otherwise we have $A\not \in \mathcal{H}$ and $\mathcal{H} \cap \Gamma^*_G(A) \ne \emptyset$. Then we can define to sets $\mathcal{H}_A = \mathcal{H} \cap \Gamma^*_G(A) = \{H_1,...,H_k\}$ and $\mathcal{H}_0 = \mathcal{H}  \backslash \mathcal{H}_A$. 
\[
	P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B} \right ) = \frac{P\left ( A \cap \left ( \bigcap_{B\in \mathcal{H}_A} \overline{B} \right ) \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right )
	}{P\left ( \bigcap_{B\in \mathcal{H}_A} \overline{B}  \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right )}.
\]

We will bound numerator and denominator. For the numerator:

\[
P\left ( A \cap \left ( \bigcap_{B\in \mathcal{H}_A} \overline{B} \right ) \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right ) \le P\left ( A \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right ) \le P(A).
\]

Where the second inequality is given by the definition of lopsidependency graph. On the other hand, for the denominator, we can define $\mathcal{H}_i := \{H_i,...,H_k\} \cup \mathcal{H}_0$.
\begin{align*}
P\left ( \bigcap_{B\in \mathcal{H}_A} \overline{B}  \Big| \bigcap_{B\in \mathcal{H}_0} \overline{B} \right )  = & \prod_{i=1}^k P\left ( \overline{B_i} \Big| \bigcap_{B\in \mathcal{H}_i} \overline{B} \right ) \\  \ge^{3.}  & \prod_{i=1}^k \left (1-\mu(H_i)\right ) 
 \ge^{4.}  \prod_{B\in\Gamma_G^*(A)} \left (1-\mu(B)\right )
\end{align*}

Where in 3. the induction hypothesis is used, and in 4. is considering  that $H_i \in \Gamma_G^*(A)$
Considering now both parts:
\[
P\left ( A \Big| \bigcap_{B\in \mathcal{H}} \overline{B} \right ) \le \frac{P(A)}{\prod_{B\in\Gamma_G^*(A)} \left (1-\mu(B)\right )} \le \mu(A).
\]
Where the last inequality uses the hypothesis on $\mu$.
\end{itemize}
\end{proof}


\begin{proof}[proof of the theorem \ref{LLL}]
\[
P\left ( \bigcap_{A\in \mathcal{A}} \overline{A}\right ) = \prod_{i=1}^m P \left( \overline{A_i} \Big | \bigcap_{j=1}^{i-1} \overline{A_j}\right) \ge^{5.} \prod_{i=1}^m ( 1 - \mu(A_i)),
\]
	where in 5. is used \ref{LemaLLL} and since $\mu:\mathcal{A}\to (0,1)$ then $P\left ( \bigcap_{A\in \mathcal{A}} \overline{A}\right )  > 0$.\\
\end{proof}


\subsection{Constructive proof of [\ref{LLLSC}]}

Moser\cite{moser2013exact} proves that there exists an algorithm such that it gives an assignment satisfying the SAT formula, should it happen that the formula satisfies \ref{LLLS} conditions. This is no a big deal, as a backtrack would be also capable of providing the solution, given that we know its existence. Not so trivial is that it would run in $O(|F|)$. We will show the version of the algorithm shown in \cite{schoning2013satisfiability}.



\begin{algorithm}
\caption{Moser's Algorithm}\label{euclid}
\begin{algorithmic}[1]
  \State $C_1,...,C_m \gets \text{Clauses in F to satisfy, globally accessible}$
  \State $\alpha \gets \text{assignment on }Var(F)$
  \State
  \Procedure{Repair}{$\alpha, C$}
  \For{$v \in Var(C)$}
  \State $\alpha(v) = \text{random} \in \{0,1\}$
  \EndFor
  \For{j := 1 to m}
  \If {$(Var(C_j)\cap Var(C) \ne \emptyset ) \wedge (C_j\alpha=0)$}
  \State Repair($C_j$)
  \EndIf
  \EndFor
  \EndProcedure
  \State
  \State Randomly choose an initial assignment $\alpha$
  \For{j := 1 to m} 
  \If{$\alpha(C_j) = 0$}
  \State Repair($C_j$)

\end{algorithmic}
\end{algorithm}


At first sight it is not clear if it terminates. If $F$ verifies \ref{LLLS} it is proved that if will end after running Repair at most  $O(\sum_{C\in F} \frac{\mu(C)}{1-\mu(C)})$


